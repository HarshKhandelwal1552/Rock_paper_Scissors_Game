{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img,array_to_img\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.externals import joblib\n",
    "from skimage.color import rgb2gray\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,Dense,MaxPooling2D,BatchNormalization,GlobalAveragePooling2D,Dropout,Flatten,GaussianNoise\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making distorted data\n",
    "datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=180,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=True,\n",
    "        validation_split=0.1)  # randomly flip images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyper parameters\n",
    "batch_size = 8\n",
    "img_width = 96\n",
    "img_hight = 96\n",
    "train_data_dir = 'new_data/'\n",
    "EPOCHS=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting data into traininga and validation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5294 images belonging to 3 classes.\n",
      "Found 587 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train = datagen.flow_from_directory(train_data_dir,\n",
    "                                    target_size = (img_width,img_hight),\n",
    "                                    batch_size = batch_size,\n",
    "                                    class_mode = 'categorical',\n",
    "                                    subset='training',\n",
    "                                    shuffle = True)\n",
    "\n",
    "val = datagen.flow_from_directory(train_data_dir,\n",
    "                                    target_size = (img_width,img_hight),\n",
    "                                    batch_size = batch_size,\n",
    "                                    class_mode = 'categorical',\n",
    "                                    subset='validation',\n",
    "                                    shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using MobileNetV2 model becaused it performed best over efficientNets, SqueezeNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    with strategy.scope():\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.applications.MobileNetV2(\n",
    "                input_shape=(img_width,img_hight, 3),\n",
    "                weights=\"imagenet\",\n",
    "                include_top=False\n",
    "            ),\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss = 'categorical_crossentropy',\n",
    "            metrics=[tf.keras.metrics.AUC(),'accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_96 (Model)  (None, 3, 3, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               163968    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,422,339\n",
      "Trainable params: 2,388,227\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saver/mobilenetv2_complete_dataset.h5',monitor = 'val_loss',verbose=2,save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing Learning rate helps the model to learn better and effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 1e-05 to 5e-05 to 1e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 403.915625 248.518125\" width=\"403.915625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       " <defs>\r\n",
       "  <style type=\"text/css\">\r\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\r\n",
       "  </style>\r\n",
       " </defs>\r\n",
       " <g id=\"figure_1\">\r\n",
       "  <g id=\"patch_1\">\r\n",
       "   <path d=\"M 0 248.518125 \r\n",
       "L 403.915625 248.518125 \r\n",
       "L 403.915625 0 \r\n",
       "L 0 0 \r\n",
       "z\r\n",
       "\" style=\"fill:none;\"/>\r\n",
       "  </g>\r\n",
       "  <g id=\"axes_1\">\r\n",
       "   <g id=\"patch_2\">\r\n",
       "    <path d=\"M 61.915625 224.64 \r\n",
       "L 396.715625 224.64 \r\n",
       "L 396.715625 7.2 \r\n",
       "L 61.915625 7.2 \r\n",
       "z\r\n",
       "\" style=\"fill:#ffffff;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_1\">\r\n",
       "    <g id=\"xtick_1\">\r\n",
       "     <g id=\"line2d_1\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L 0 3.5 \r\n",
       "\" id=\"m405f7ebbde\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"77.133807\" xlink:href=\"#m405f7ebbde\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_1\">\r\n",
       "      <!-- 0 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 31.78125 66.40625 \r\n",
       "Q 24.171875 66.40625 20.328125 58.90625 \r\n",
       "Q 16.5 51.421875 16.5 36.375 \r\n",
       "Q 16.5 21.390625 20.328125 13.890625 \r\n",
       "Q 24.171875 6.390625 31.78125 6.390625 \r\n",
       "Q 39.453125 6.390625 43.28125 13.890625 \r\n",
       "Q 47.125 21.390625 47.125 36.375 \r\n",
       "Q 47.125 51.421875 43.28125 58.90625 \r\n",
       "Q 39.453125 66.40625 31.78125 66.40625 \r\n",
       "z\r\n",
       "M 31.78125 74.21875 \r\n",
       "Q 44.046875 74.21875 50.515625 64.515625 \r\n",
       "Q 56.984375 54.828125 56.984375 36.375 \r\n",
       "Q 56.984375 17.96875 50.515625 8.265625 \r\n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \r\n",
       "Q 6.59375 17.96875 6.59375 36.375 \r\n",
       "Q 6.59375 54.828125 13.0625 64.515625 \r\n",
       "Q 19.53125 74.21875 31.78125 74.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-48\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(73.952557 239.238438)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_2\">\r\n",
       "     <g id=\"line2d_2\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"138.62141\" xlink:href=\"#m405f7ebbde\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_2\">\r\n",
       "      <!-- 20 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 19.1875 8.296875 \r\n",
       "L 53.609375 8.296875 \r\n",
       "L 53.609375 0 \r\n",
       "L 7.328125 0 \r\n",
       "L 7.328125 8.296875 \r\n",
       "Q 12.9375 14.109375 22.625 23.890625 \r\n",
       "Q 32.328125 33.6875 34.8125 36.53125 \r\n",
       "Q 39.546875 41.84375 41.421875 45.53125 \r\n",
       "Q 43.3125 49.21875 43.3125 52.78125 \r\n",
       "Q 43.3125 58.59375 39.234375 62.25 \r\n",
       "Q 35.15625 65.921875 28.609375 65.921875 \r\n",
       "Q 23.96875 65.921875 18.8125 64.3125 \r\n",
       "Q 13.671875 62.703125 7.8125 59.421875 \r\n",
       "L 7.8125 69.390625 \r\n",
       "Q 13.765625 71.78125 18.9375 73 \r\n",
       "Q 24.125 74.21875 28.421875 74.21875 \r\n",
       "Q 39.75 74.21875 46.484375 68.546875 \r\n",
       "Q 53.21875 62.890625 53.21875 53.421875 \r\n",
       "Q 53.21875 48.921875 51.53125 44.890625 \r\n",
       "Q 49.859375 40.875 45.40625 35.40625 \r\n",
       "Q 44.1875 33.984375 37.640625 27.21875 \r\n",
       "Q 31.109375 20.453125 19.1875 8.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-50\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(132.25891 239.238438)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_3\">\r\n",
       "     <g id=\"line2d_3\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"200.109013\" xlink:href=\"#m405f7ebbde\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_3\">\r\n",
       "      <!-- 40 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 37.796875 64.3125 \r\n",
       "L 12.890625 25.390625 \r\n",
       "L 37.796875 25.390625 \r\n",
       "z\r\n",
       "M 35.203125 72.90625 \r\n",
       "L 47.609375 72.90625 \r\n",
       "L 47.609375 25.390625 \r\n",
       "L 58.015625 25.390625 \r\n",
       "L 58.015625 17.1875 \r\n",
       "L 47.609375 17.1875 \r\n",
       "L 47.609375 0 \r\n",
       "L 37.796875 0 \r\n",
       "L 37.796875 17.1875 \r\n",
       "L 4.890625 17.1875 \r\n",
       "L 4.890625 26.703125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-52\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(193.746513 239.238438)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_4\">\r\n",
       "     <g id=\"line2d_4\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"261.596617\" xlink:href=\"#m405f7ebbde\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_4\">\r\n",
       "      <!-- 60 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 33.015625 40.375 \r\n",
       "Q 26.375 40.375 22.484375 35.828125 \r\n",
       "Q 18.609375 31.296875 18.609375 23.390625 \r\n",
       "Q 18.609375 15.53125 22.484375 10.953125 \r\n",
       "Q 26.375 6.390625 33.015625 6.390625 \r\n",
       "Q 39.65625 6.390625 43.53125 10.953125 \r\n",
       "Q 47.40625 15.53125 47.40625 23.390625 \r\n",
       "Q 47.40625 31.296875 43.53125 35.828125 \r\n",
       "Q 39.65625 40.375 33.015625 40.375 \r\n",
       "z\r\n",
       "M 52.59375 71.296875 \r\n",
       "L 52.59375 62.3125 \r\n",
       "Q 48.875 64.0625 45.09375 64.984375 \r\n",
       "Q 41.3125 65.921875 37.59375 65.921875 \r\n",
       "Q 27.828125 65.921875 22.671875 59.328125 \r\n",
       "Q 17.53125 52.734375 16.796875 39.40625 \r\n",
       "Q 19.671875 43.65625 24.015625 45.921875 \r\n",
       "Q 28.375 48.1875 33.59375 48.1875 \r\n",
       "Q 44.578125 48.1875 50.953125 41.515625 \r\n",
       "Q 57.328125 34.859375 57.328125 23.390625 \r\n",
       "Q 57.328125 12.15625 50.6875 5.359375 \r\n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \r\n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \r\n",
       "Q 6.984375 17.96875 6.984375 36.375 \r\n",
       "Q 6.984375 53.65625 15.1875 63.9375 \r\n",
       "Q 23.390625 74.21875 37.203125 74.21875 \r\n",
       "Q 40.921875 74.21875 44.703125 73.484375 \r\n",
       "Q 48.484375 72.75 52.59375 71.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-54\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(255.234117 239.238438)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_5\">\r\n",
       "     <g id=\"line2d_5\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"323.08422\" xlink:href=\"#m405f7ebbde\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_5\">\r\n",
       "      <!-- 80 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 31.78125 34.625 \r\n",
       "Q 24.75 34.625 20.71875 30.859375 \r\n",
       "Q 16.703125 27.09375 16.703125 20.515625 \r\n",
       "Q 16.703125 13.921875 20.71875 10.15625 \r\n",
       "Q 24.75 6.390625 31.78125 6.390625 \r\n",
       "Q 38.8125 6.390625 42.859375 10.171875 \r\n",
       "Q 46.921875 13.96875 46.921875 20.515625 \r\n",
       "Q 46.921875 27.09375 42.890625 30.859375 \r\n",
       "Q 38.875 34.625 31.78125 34.625 \r\n",
       "z\r\n",
       "M 21.921875 38.8125 \r\n",
       "Q 15.578125 40.375 12.03125 44.71875 \r\n",
       "Q 8.5 49.078125 8.5 55.328125 \r\n",
       "Q 8.5 64.0625 14.71875 69.140625 \r\n",
       "Q 20.953125 74.21875 31.78125 74.21875 \r\n",
       "Q 42.671875 74.21875 48.875 69.140625 \r\n",
       "Q 55.078125 64.0625 55.078125 55.328125 \r\n",
       "Q 55.078125 49.078125 51.53125 44.71875 \r\n",
       "Q 48 40.375 41.703125 38.8125 \r\n",
       "Q 48.828125 37.15625 52.796875 32.3125 \r\n",
       "Q 56.78125 27.484375 56.78125 20.515625 \r\n",
       "Q 56.78125 9.90625 50.3125 4.234375 \r\n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.734375 -1.421875 13.25 4.234375 \r\n",
       "Q 6.78125 9.90625 6.78125 20.515625 \r\n",
       "Q 6.78125 27.484375 10.78125 32.3125 \r\n",
       "Q 14.796875 37.15625 21.921875 38.8125 \r\n",
       "z\r\n",
       "M 18.3125 54.390625 \r\n",
       "Q 18.3125 48.734375 21.84375 45.5625 \r\n",
       "Q 25.390625 42.390625 31.78125 42.390625 \r\n",
       "Q 38.140625 42.390625 41.71875 45.5625 \r\n",
       "Q 45.3125 48.734375 45.3125 54.390625 \r\n",
       "Q 45.3125 60.0625 41.71875 63.234375 \r\n",
       "Q 38.140625 66.40625 31.78125 66.40625 \r\n",
       "Q 25.390625 66.40625 21.84375 63.234375 \r\n",
       "Q 18.3125 60.0625 18.3125 54.390625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-56\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(316.72172 239.238438)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-56\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_6\">\r\n",
       "     <g id=\"line2d_6\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"384.571823\" xlink:href=\"#m405f7ebbde\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_6\">\r\n",
       "      <!-- 100 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 12.40625 8.296875 \r\n",
       "L 28.515625 8.296875 \r\n",
       "L 28.515625 63.921875 \r\n",
       "L 10.984375 60.40625 \r\n",
       "L 10.984375 69.390625 \r\n",
       "L 28.421875 72.90625 \r\n",
       "L 38.28125 72.90625 \r\n",
       "L 38.28125 8.296875 \r\n",
       "L 54.390625 8.296875 \r\n",
       "L 54.390625 0 \r\n",
       "L 12.40625 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-49\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(375.028073 239.238438)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_2\">\r\n",
       "    <g id=\"ytick_1\">\r\n",
       "     <g id=\"line2d_7\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L -3.5 0 \r\n",
       "\" id=\"mbae504d236\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.915625\" xlink:href=\"#mbae504d236\" y=\"214.756364\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_7\">\r\n",
       "      <!-- 0.000010 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 10.6875 12.40625 \r\n",
       "L 21 12.40625 \r\n",
       "L 21 0 \r\n",
       "L 10.6875 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-46\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 218.555582)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"413.525391\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_2\">\r\n",
       "     <g id=\"line2d_8\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.915625\" xlink:href=\"#mbae504d236\" y=\"190.047273\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_8\">\r\n",
       "      <!-- 0.000015 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 10.796875 72.90625 \r\n",
       "L 49.515625 72.90625 \r\n",
       "L 49.515625 64.59375 \r\n",
       "L 19.828125 64.59375 \r\n",
       "L 19.828125 46.734375 \r\n",
       "Q 21.96875 47.46875 24.109375 47.828125 \r\n",
       "Q 26.265625 48.1875 28.421875 48.1875 \r\n",
       "Q 40.625 48.1875 47.75 41.5 \r\n",
       "Q 54.890625 34.8125 54.890625 23.390625 \r\n",
       "Q 54.890625 11.625 47.5625 5.09375 \r\n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \r\n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \r\n",
       "Q 12.796875 0.140625 7.71875 1.703125 \r\n",
       "L 7.71875 11.625 \r\n",
       "Q 12.109375 9.234375 16.796875 8.0625 \r\n",
       "Q 21.484375 6.890625 26.703125 6.890625 \r\n",
       "Q 35.15625 6.890625 40.078125 11.328125 \r\n",
       "Q 45.015625 15.765625 45.015625 23.390625 \r\n",
       "Q 45.015625 31 40.078125 35.4375 \r\n",
       "Q 35.15625 39.890625 26.703125 39.890625 \r\n",
       "Q 22.75 39.890625 18.8125 39.015625 \r\n",
       "Q 14.890625 38.140625 10.796875 36.28125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-53\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 193.846491)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"413.525391\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_3\">\r\n",
       "     <g id=\"line2d_9\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.915625\" xlink:href=\"#mbae504d236\" y=\"165.338182\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_9\">\r\n",
       "      <!-- 0.000020 -->\r\n",
       "      <g transform=\"translate(7.2 169.137401)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"413.525391\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_4\">\r\n",
       "     <g id=\"line2d_10\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.915625\" xlink:href=\"#mbae504d236\" y=\"140.629091\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_10\">\r\n",
       "      <!-- 0.000025 -->\r\n",
       "      <g transform=\"translate(7.2 144.42831)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"413.525391\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_5\">\r\n",
       "     <g id=\"line2d_11\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.915625\" xlink:href=\"#mbae504d236\" y=\"115.92\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_11\">\r\n",
       "      <!-- 0.000030 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 40.578125 39.3125 \r\n",
       "Q 47.65625 37.796875 51.625 33 \r\n",
       "Q 55.609375 28.21875 55.609375 21.1875 \r\n",
       "Q 55.609375 10.40625 48.1875 4.484375 \r\n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \r\n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \r\n",
       "Q 12.796875 0.390625 7.625 2.203125 \r\n",
       "L 7.625 11.71875 \r\n",
       "Q 11.71875 9.328125 16.59375 8.109375 \r\n",
       "Q 21.484375 6.890625 26.8125 6.890625 \r\n",
       "Q 36.078125 6.890625 40.9375 10.546875 \r\n",
       "Q 45.796875 14.203125 45.796875 21.1875 \r\n",
       "Q 45.796875 27.640625 41.28125 31.265625 \r\n",
       "Q 36.765625 34.90625 28.71875 34.90625 \r\n",
       "L 20.21875 34.90625 \r\n",
       "L 20.21875 43.015625 \r\n",
       "L 29.109375 43.015625 \r\n",
       "Q 36.375 43.015625 40.234375 45.921875 \r\n",
       "Q 44.09375 48.828125 44.09375 54.296875 \r\n",
       "Q 44.09375 59.90625 40.109375 62.90625 \r\n",
       "Q 36.140625 65.921875 28.71875 65.921875 \r\n",
       "Q 24.65625 65.921875 20.015625 65.03125 \r\n",
       "Q 15.375 64.15625 9.8125 62.3125 \r\n",
       "L 9.8125 71.09375 \r\n",
       "Q 15.4375 72.65625 20.34375 73.4375 \r\n",
       "Q 25.25 74.21875 29.59375 74.21875 \r\n",
       "Q 40.828125 74.21875 47.359375 69.109375 \r\n",
       "Q 53.90625 64.015625 53.90625 55.328125 \r\n",
       "Q 53.90625 49.265625 50.4375 45.09375 \r\n",
       "Q 46.96875 40.921875 40.578125 39.3125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-51\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 119.719219)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-51\"/>\r\n",
       "       <use x=\"413.525391\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_6\">\r\n",
       "     <g id=\"line2d_12\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.915625\" xlink:href=\"#mbae504d236\" y=\"91.210909\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_12\">\r\n",
       "      <!-- 0.000035 -->\r\n",
       "      <g transform=\"translate(7.2 95.010128)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-51\"/>\r\n",
       "       <use x=\"413.525391\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_7\">\r\n",
       "     <g id=\"line2d_13\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.915625\" xlink:href=\"#mbae504d236\" y=\"66.501818\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_13\">\r\n",
       "      <!-- 0.000040 -->\r\n",
       "      <g transform=\"translate(7.2 70.301037)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"413.525391\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_8\">\r\n",
       "     <g id=\"line2d_14\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.915625\" xlink:href=\"#mbae504d236\" y=\"41.792727\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_14\">\r\n",
       "      <!-- 0.000045 -->\r\n",
       "      <g transform=\"translate(7.2 45.591946)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"413.525391\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_9\">\r\n",
       "     <g id=\"line2d_15\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.915625\" xlink:href=\"#mbae504d236\" y=\"17.083636\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_15\">\r\n",
       "      <!-- 0.000050 -->\r\n",
       "      <g transform=\"translate(7.2 20.882855)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"413.525391\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_16\">\r\n",
       "    <path clip-path=\"url(#pcb88499905)\" d=\"M 77.133807 214.756364 \r\n",
       "L 80.208187 175.221818 \r\n",
       "L 83.282567 135.687273 \r\n",
       "L 86.356947 96.152727 \r\n",
       "L 89.431327 56.618182 \r\n",
       "L 92.505708 17.083636 \r\n",
       "L 95.580088 56.618182 \r\n",
       "L 98.654468 88.245818 \r\n",
       "L 101.728848 113.547927 \r\n",
       "L 104.803228 133.789615 \r\n",
       "L 107.877608 149.982964 \r\n",
       "L 110.951989 162.937644 \r\n",
       "L 114.026369 173.301388 \r\n",
       "L 117.100749 181.592383 \r\n",
       "L 120.175129 188.225179 \r\n",
       "L 123.249509 193.531416 \r\n",
       "L 126.323889 197.776406 \r\n",
       "L 129.39827 201.172397 \r\n",
       "L 132.47265 203.889191 \r\n",
       "L 135.54703 206.062625 \r\n",
       "L 138.62141 207.801373 \r\n",
       "L 141.69579 209.192371 \r\n",
       "L 144.77017 210.30517 \r\n",
       "L 147.844551 211.195408 \r\n",
       "L 150.918931 211.907599 \r\n",
       "L 153.993311 212.477352 \r\n",
       "L 157.067691 212.933155 \r\n",
       "L 160.142071 213.297796 \r\n",
       "L 163.216451 213.58951 \r\n",
       "L 166.290832 213.822881 \r\n",
       "L 169.365212 214.009577 \r\n",
       "L 172.439592 214.158934 \r\n",
       "L 175.513972 214.27842 \r\n",
       "L 178.588352 214.374009 \r\n",
       "L 181.662732 214.45048 \r\n",
       "L 184.737113 214.511657 \r\n",
       "L 187.811493 214.560598 \r\n",
       "L 190.885873 214.599751 \r\n",
       "L 193.960253 214.631074 \r\n",
       "L 197.034633 214.656132 \r\n",
       "L 200.109013 214.676178 \r\n",
       "L 203.183394 214.692215 \r\n",
       "L 206.257774 214.705045 \r\n",
       "L 209.332154 214.715309 \r\n",
       "L 212.406534 214.72352 \r\n",
       "L 215.480914 214.730088 \r\n",
       "L 218.555294 214.735343 \r\n",
       "L 221.629675 214.739548 \r\n",
       "L 224.704055 214.742911 \r\n",
       "L 227.778435 214.745601 \r\n",
       "L 230.852815 214.747754 \r\n",
       "L 233.927195 214.749476 \r\n",
       "L 237.001575 214.750853 \r\n",
       "L 240.075956 214.751955 \r\n",
       "L 243.150336 214.752837 \r\n",
       "L 246.224716 214.753542 \r\n",
       "L 249.299096 214.754107 \r\n",
       "L 252.373476 214.754558 \r\n",
       "L 255.447856 214.754919 \r\n",
       "L 258.522237 214.755208 \r\n",
       "L 261.596617 214.755439 \r\n",
       "L 264.670997 214.755624 \r\n",
       "L 267.745377 214.755772 \r\n",
       "L 270.819757 214.75589 \r\n",
       "L 273.894137 214.755985 \r\n",
       "L 276.968518 214.756061 \r\n",
       "L 280.042898 214.756121 \r\n",
       "L 283.117278 214.75617 \r\n",
       "L 286.191658 214.756209 \r\n",
       "L 289.266038 214.75624 \r\n",
       "L 292.340418 214.756264 \r\n",
       "L 295.414799 214.756284 \r\n",
       "L 298.489179 214.7563 \r\n",
       "L 301.563559 214.756313 \r\n",
       "L 304.637939 214.756323 \r\n",
       "L 307.712319 214.756331 \r\n",
       "L 310.786699 214.756338 \r\n",
       "L 313.86108 214.756343 \r\n",
       "L 316.93546 214.756347 \r\n",
       "L 320.00984 214.75635 \r\n",
       "L 323.08422 214.756353 \r\n",
       "L 326.1586 214.756355 \r\n",
       "L 329.23298 214.756357 \r\n",
       "L 332.307361 214.756358 \r\n",
       "L 335.381741 214.756359 \r\n",
       "L 338.456121 214.75636 \r\n",
       "L 341.530501 214.756361 \r\n",
       "L 344.604881 214.756361 \r\n",
       "L 347.679261 214.756362 \r\n",
       "L 350.753642 214.756362 \r\n",
       "L 353.828022 214.756362 \r\n",
       "L 356.902402 214.756363 \r\n",
       "L 359.976782 214.756363 \r\n",
       "L 363.051162 214.756363 \r\n",
       "L 366.125542 214.756363 \r\n",
       "L 369.199923 214.756363 \r\n",
       "L 372.274303 214.756363 \r\n",
       "L 375.348683 214.756363 \r\n",
       "L 378.423063 214.756363 \r\n",
       "L 381.497443 214.756363 \r\n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_3\">\r\n",
       "    <path d=\"M 61.915625 224.64 \r\n",
       "L 61.915625 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_4\">\r\n",
       "    <path d=\"M 396.715625 224.64 \r\n",
       "L 396.715625 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_5\">\r\n",
       "    <path d=\"M 61.915625 224.64 \r\n",
       "L 396.715625 224.64 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_6\">\r\n",
       "    <path d=\"M 61.915625 7.2 \r\n",
       "L 396.715625 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "  </g>\r\n",
       " </g>\r\n",
       " <defs>\r\n",
       "  <clipPath id=\"pcb88499905\">\r\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"61.915625\" y=\"7.2\"/>\r\n",
       "  </clipPath>\r\n",
       " </defs>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_lrfn(lr_start=0.00001, lr_max=0.00005, \n",
    "               lr_min=0.00001, lr_rampup_epochs=5, \n",
    "               lr_sustain_epochs=0, lr_exp_decay=.8):\n",
    "    lr_max = lr_max * strategy.num_replicas_in_sync\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) *\\\n",
    "                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n",
    "                                - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "    return lrfn\n",
    "\n",
    "lrfn = build_lrfn()\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n",
    "rng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\n",
    "y = [lrfn(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using twice the data by using Image Data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_steps_per_epoch = 2*(len(train) // batch_size)\n",
    "val_steps_per_epoch = 2*(len(val) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 1.0000 - val_accuracy: 1.0000 - lr: 1.0001e-05\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.0000713623846353e-05.\n",
      "Epoch 55/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0853 - auc: 0.9885 - accuracy: 0.9718\n",
      "Epoch 00055: val_loss did not improve from 0.00171\n",
      "164/164 [==============================] - 10s 64ms/step - loss: 0.0853 - auc: 0.9885 - accuracy: 0.9718 - val_loss: 0.0068 - val_auc: 0.9998 - val_accuracy: 1.0000 - lr: 1.0001e-05\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0000570899077083e-05.\n",
      "Epoch 56/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0815 - auc: 0.9895 - accuracy: 0.9726\n",
      "Epoch 00056: val_loss did not improve from 0.00171\n",
      "164/164 [==============================] - 10s 60ms/step - loss: 0.0815 - auc: 0.9895 - accuracy: 0.9726 - val_loss: 0.0148 - val_auc: 0.9997 - val_accuracy: 0.9931 - lr: 1.0001e-05\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.0000456719261668e-05.\n",
      "Epoch 57/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.1021 - auc: 0.9864 - accuracy: 0.9611\n",
      "Epoch 00057: val_loss did not improve from 0.00171\n",
      "164/164 [==============================] - 10s 60ms/step - loss: 0.1021 - auc: 0.9864 - accuracy: 0.9611 - val_loss: 0.0085 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0000365375409333e-05.\n",
      "Epoch 58/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0622 - auc: 0.9939 - accuracy: 0.9848\n",
      "Epoch 00058: val_loss did not improve from 0.00171\n",
      "164/164 [==============================] - 10s 63ms/step - loss: 0.0622 - auc: 0.9939 - accuracy: 0.9848 - val_loss: 0.0049 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.0000292300327467e-05.\n",
      "Epoch 59/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0653 - auc: 0.9932 - accuracy: 0.9809\n",
      "Epoch 00059: val_loss did not improve from 0.00171\n",
      "164/164 [==============================] - 10s 60ms/step - loss: 0.0653 - auc: 0.9932 - accuracy: 0.9809 - val_loss: 0.0051 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0000233840261974e-05.\n",
      "Epoch 60/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0687 - auc: 0.9925 - accuracy: 0.9787\n",
      "Epoch 00060: val_loss did not improve from 0.00171\n",
      "164/164 [==============================] - 10s 60ms/step - loss: 0.0687 - auc: 0.9925 - accuracy: 0.9787 - val_loss: 0.0193 - val_auc: 0.9964 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 1.000018707220958e-05.\n",
      "Epoch 61/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0584 - auc: 0.9931 - accuracy: 0.9817\n",
      "Epoch 00061: val_loss did not improve from 0.00171\n",
      "164/164 [==============================] - 10s 63ms/step - loss: 0.0584 - auc: 0.9931 - accuracy: 0.9817 - val_loss: 0.0048 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 1.0000149657767664e-05.\n",
      "Epoch 62/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0726 - auc: 0.9923 - accuracy: 0.9764\n",
      "Epoch 00062: val_loss did not improve from 0.00171\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0726 - auc: 0.9923 - accuracy: 0.9764 - val_loss: 0.0098 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 1.000011972621413e-05.\n",
      "Epoch 63/100\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.0812 - auc: 0.9890 - accuracy: 0.9716\n",
      "Epoch 00063: val_loss improved from 0.00171 to 0.00087, saving model to saver/mobilenetv2_complete_dataset.h5\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0807 - auc: 0.9891 - accuracy: 0.9718 - val_loss: 8.7031e-04 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 1.0000095780971305e-05.\n",
      "Epoch 64/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0773 - auc: 0.9880 - accuracy: 0.9733\n",
      "Epoch 00064: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 63ms/step - loss: 0.0773 - auc: 0.9880 - accuracy: 0.9733 - val_loss: 0.0014 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 1.0000076624777044e-05.\n",
      "Epoch 65/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0595 - auc: 0.9919 - accuracy: 0.9794\n",
      "Epoch 00065: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 59ms/step - loss: 0.0595 - auc: 0.9919 - accuracy: 0.9794 - val_loss: 0.0050 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 1.0000061299821636e-05.\n",
      "Epoch 66/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0560 - auc: 0.9917 - accuracy: 0.9825\n",
      "Epoch 00066: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 63ms/step - loss: 0.0560 - auc: 0.9917 - accuracy: 0.9825 - val_loss: 0.0070 - val_auc: 1.0000 - val_accuracy: 0.9931 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 1.0000049039857308e-05.\n",
      "Epoch 67/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0653 - auc: 0.9904 - accuracy: 0.9733\n",
      "Epoch 00067: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 60ms/step - loss: 0.0653 - auc: 0.9904 - accuracy: 0.9733 - val_loss: 0.0033 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 1.0000039231885846e-05.\n",
      "Epoch 68/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0567 - auc: 0.9931 - accuracy: 0.9809\n",
      "Epoch 00068: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 61ms/step - loss: 0.0567 - auc: 0.9931 - accuracy: 0.9809 - val_loss: 0.0021 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 1.0000031385508678e-05.\n",
      "Epoch 69/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0644 - auc: 0.9902 - accuracy: 0.9779\n",
      "Epoch 00069: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0644 - auc: 0.9902 - accuracy: 0.9779 - val_loss: 0.0015 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 1.0000025108406942e-05.\n",
      "Epoch 70/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0807 - auc: 0.9903 - accuracy: 0.9764\n",
      "Epoch 00070: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 63ms/step - loss: 0.0807 - auc: 0.9903 - accuracy: 0.9764 - val_loss: 0.0130 - val_auc: 0.9999 - val_accuracy: 0.9931 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 1.0000020086725554e-05.\n",
      "Epoch 71/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0584 - auc: 0.9915 - accuracy: 0.9802\n",
      "Epoch 00071: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 61ms/step - loss: 0.0584 - auc: 0.9915 - accuracy: 0.9802 - val_loss: 0.0073 - val_auc: 0.9997 - val_accuracy: 0.9928 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 1.0000016069380443e-05.\n",
      "Epoch 72/100\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.0454 - auc: 0.9946 - accuracy: 0.9862\n",
      "Epoch 00072: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 60ms/step - loss: 0.0452 - auc: 0.9946 - accuracy: 0.9863 - val_loss: 0.0087 - val_auc: 1.0000 - val_accuracy: 0.9931 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 1.0000012855504354e-05.\n",
      "Epoch 73/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0630 - auc: 0.9925 - accuracy: 0.9802\n",
      "Epoch 00073: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0630 - auc: 0.9925 - accuracy: 0.9802 - val_loss: 0.0029 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 1.0000010284403484e-05.\n",
      "Epoch 74/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0551 - auc: 0.9944 - accuracy: 0.9817\n",
      "Epoch 00074: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 61ms/step - loss: 0.0551 - auc: 0.9944 - accuracy: 0.9817 - val_loss: 0.0016 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 1.0000008227522788e-05.\n",
      "Epoch 75/100\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.0639 - auc: 0.9923 - accuracy: 0.9770\n",
      "Epoch 00075: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0636 - auc: 0.9924 - accuracy: 0.9771 - val_loss: 0.0024 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 1.000000658201823e-05.\n",
      "Epoch 76/100\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.0642 - auc: 0.9916 - accuracy: 0.9801\n",
      "Epoch 00076: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 61ms/step - loss: 0.0638 - auc: 0.9917 - accuracy: 0.9802 - val_loss: 0.0018 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 1.0000005265614585e-05.\n",
      "Epoch 77/100\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.0559 - auc: 0.9934 - accuracy: 0.9823\n",
      "Epoch 00077: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0557 - auc: 0.9934 - accuracy: 0.9824 - val_loss: 0.0023 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 1.0000004212491667e-05.\n",
      "Epoch 78/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0517 - auc: 0.9937 - accuracy: 0.9825\n",
      "Epoch 00078: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0517 - auc: 0.9937 - accuracy: 0.9825 - val_loss: 0.0034 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 1.0000003369993335e-05.\n",
      "Epoch 79/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0486 - auc: 0.9930 - accuracy: 0.9855\n",
      "Epoch 00079: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0486 - auc: 0.9930 - accuracy: 0.9855 - val_loss: 0.0041 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 1.0000002695994668e-05.\n",
      "Epoch 80/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0729 - auc: 0.9885 - accuracy: 0.9733\n",
      "Epoch 00080: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 60ms/step - loss: 0.0729 - auc: 0.9885 - accuracy: 0.9733 - val_loss: 0.0021 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 1.0000002156795734e-05.\n",
      "Epoch 81/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0725 - auc: 0.9897 - accuracy: 0.9779\n",
      "Epoch 00081: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 63ms/step - loss: 0.0725 - auc: 0.9897 - accuracy: 0.9779 - val_loss: 0.0017 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 1.0000001725436587e-05.\n",
      "Epoch 82/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0525 - auc: 0.9917 - accuracy: 0.9832\n",
      "Epoch 00082: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0525 - auc: 0.9917 - accuracy: 0.9832 - val_loss: 0.0052 - val_auc: 0.9999 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 1.0000001380349271e-05.\n",
      "Epoch 83/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0599 - auc: 0.9916 - accuracy: 0.9779\n",
      "Epoch 00083: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0599 - auc: 0.9916 - accuracy: 0.9779 - val_loss: 0.0012 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 1.0000001104279416e-05.\n",
      "Epoch 84/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0517 - auc: 0.9917 - accuracy: 0.9825\n",
      "Epoch 00084: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 61ms/step - loss: 0.0517 - auc: 0.9917 - accuracy: 0.9825 - val_loss: 0.0031 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 1.0000000883423533e-05.\n",
      "Epoch 85/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0543 - auc: 0.9928 - accuracy: 0.9802\n",
      "Epoch 00085: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 11s 65ms/step - loss: 0.0543 - auc: 0.9928 - accuracy: 0.9802 - val_loss: 0.0110 - val_auc: 0.9998 - val_accuracy: 0.9931 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 1.0000000706738827e-05.\n",
      "Epoch 86/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0625 - auc: 0.9912 - accuracy: 0.9832\n",
      "Epoch 00086: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0625 - auc: 0.9912 - accuracy: 0.9832 - val_loss: 0.0099 - val_auc: 0.9964 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 1.0000000565391061e-05.\n",
      "Epoch 87/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0445 - auc: 0.9923 - accuracy: 0.9848\n",
      "Epoch 00087: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 61ms/step - loss: 0.0445 - auc: 0.9923 - accuracy: 0.9848 - val_loss: 0.0120 - val_auc: 0.9998 - val_accuracy: 0.9931 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 1.0000000452312849e-05.\n",
      "Epoch 88/100\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.0397 - auc: 0.9927 - accuracy: 0.9900\n",
      "Epoch 00088: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 63ms/step - loss: 0.0395 - auc: 0.9928 - accuracy: 0.9901 - val_loss: 0.0012 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 1.000000036185028e-05.\n",
      "Epoch 89/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0443 - auc: 0.9950 - accuracy: 0.9855\n",
      "Epoch 00089: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 61ms/step - loss: 0.0443 - auc: 0.9950 - accuracy: 0.9855 - val_loss: 0.0088 - val_auc: 0.9999 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 1.0000000289480224e-05.\n",
      "Epoch 90/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0352 - auc: 0.9939 - accuracy: 0.9893\n",
      "Epoch 00090: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0352 - auc: 0.9939 - accuracy: 0.9893 - val_loss: 0.0021 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 1.0000000231584179e-05.\n",
      "Epoch 91/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0359 - auc: 0.9928 - accuracy: 0.9870\n",
      "Epoch 00091: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 61ms/step - loss: 0.0359 - auc: 0.9928 - accuracy: 0.9870 - val_loss: 0.0014 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 1.0000000185267343e-05.\n",
      "Epoch 92/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0495 - auc: 0.9936 - accuracy: 0.9825\n",
      "Epoch 00092: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 10s 61ms/step - loss: 0.0495 - auc: 0.9936 - accuracy: 0.9825 - val_loss: 0.0034 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 1.0000000148213875e-05.\n",
      "Epoch 93/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0552 - auc: 0.9910 - accuracy: 0.9763\n",
      "Epoch 00093: val_loss did not improve from 0.00087\n",
      "164/164 [==============================] - 12s 71ms/step - loss: 0.0552 - auc: 0.9910 - accuracy: 0.9763 - val_loss: 0.0010 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 1.00000001185711e-05.\n",
      "Epoch 94/100\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.0269 - auc: 0.9939 - accuracy: 0.9908\n",
      "Epoch 00094: val_loss improved from 0.00087 to 0.00062, saving model to saver/mobilenetv2_complete_dataset.h5\n",
      "164/164 [==============================] - 12s 71ms/step - loss: 0.0269 - auc: 0.9939 - accuracy: 0.9909 - val_loss: 6.1737e-04 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 1.0000000094856881e-05.\n",
      "Epoch 95/100\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.0678 - auc: 0.9885 - accuracy: 0.9770\n",
      "Epoch 00095: val_loss did not improve from 0.00062\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0674 - auc: 0.9886 - accuracy: 0.9771 - val_loss: 0.0085 - val_auc: 0.9997 - val_accuracy: 0.9931 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 1.0000000075885505e-05.\n",
      "Epoch 96/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0450 - auc: 0.9932 - accuracy: 0.9817\n",
      "Epoch 00096: val_loss did not improve from 0.00062\n",
      "164/164 [==============================] - 10s 64ms/step - loss: 0.0450 - auc: 0.9932 - accuracy: 0.9817 - val_loss: 8.0603e-04 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 1.0000000060708404e-05.\n",
      "Epoch 97/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0407 - auc: 0.9928 - accuracy: 0.9878\n",
      "Epoch 00097: val_loss did not improve from 0.00062\n",
      "164/164 [==============================] - 10s 63ms/step - loss: 0.0407 - auc: 0.9928 - accuracy: 0.9878 - val_loss: 0.0018 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 1.0000000048566724e-05.\n",
      "Epoch 98/100\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.0362 - auc: 0.9922 - accuracy: 0.9862\n",
      "Epoch 00098: val_loss did not improve from 0.00062\n",
      "164/164 [==============================] - 10s 62ms/step - loss: 0.0360 - auc: 0.9922 - accuracy: 0.9863 - val_loss: 0.0037 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 1.0000000038853378e-05.\n",
      "Epoch 99/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0338 - auc: 0.9943 - accuracy: 0.9886\n",
      "Epoch 00099: val_loss did not improve from 0.00062\n",
      "164/164 [==============================] - 11s 64ms/step - loss: 0.0338 - auc: 0.9943 - accuracy: 0.9886 - val_loss: 0.0014 - val_auc: 1.0000 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 1.0000000031082703e-05.\n",
      "Epoch 100/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.0527 - auc: 0.9908 - accuracy: 0.9878\n",
      "Epoch 00100: val_loss did not improve from 0.00062\n",
      "164/164 [==============================] - 10s 63ms/step - loss: 0.0527 - auc: 0.9908 - accuracy: 0.9878 - val_loss: 0.0083 - val_auc: 0.9999 - val_accuracy: 0.9931 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train,\n",
    "                    steps_per_epoch= train_steps_per_epoch,\n",
    "                    epochs= EPOCHS,\n",
    "                    validation_data= val ,\n",
    "                    validation_steps= val_steps_per_epoch,\n",
    "                    callbacks=[lr_schedule, checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving our Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "\n",
    "with open('saver/mobilenetv2_complete_dataset.json','w') as json_file:\n",
    "\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
